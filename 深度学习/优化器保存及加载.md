1. **保存优化器状态**:

   ```python
   import torch.optim as optim
   
   model = # your model
   optimizer = optim.Adam(model.parameters(), lr=0.001)
   
   # Train the model
   for epoch in range(num_epochs):
       # Training code...
       optimizer.step()
   
   # Save the optimizer state
   torch.save(optimizer.state_dict(), 'optimizer.pth')
   ```

2. **加载优化器状态**:

   ```python
   import torch.optim as optim
   
   model = # your model
   optimizer = optim.Adam(model.parameters(), lr=0.001)
   
   # Load the optimizer state
   optimizer.load_state_dict(torch.load('optimizer.pth'))
   
   # Continue training the model
   for epoch in range(num_epochs):
       # Training code...
       optimizer.step()
   ```

在上述示例中:

1. 训练过程中,我们定义了一个 Adam 优化器,并在每个训练步骤中更新模型参数。
2. 当训练完成后,我们使用 `torch.save()` 函数保存优化器的状态字典到文件 `'optimizer.pth'` 中。
3. 当需要恢复训练时,我们重新定义相同的优化器,并使用 `torch.load()` 函数加载保存的优化器状态字典。
4. 然后继续训练模型,优化器会从上次保存的状态开始更新。

这样做的好处是:

- 可以在训练过程中保存优化器的状态,避免从头开始训练。
- 当需要继续训练时,可以快速恢复之前的训练进度。
- 可以在不同的训练环境中共享和加载优化器的状态。

除了保存优化器状态,你也可以同时保存模型的参数,以便于整个模型的保存和加载。这样可以确保在恢复训练时,模型和优化器的状态都能完全恢复到之前的状态。
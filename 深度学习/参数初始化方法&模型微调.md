### 参数初始化

```python
# 初始化方法1
from torch.nn import functional as F
from torch import nn


class net(nn.Module):
    def __init__(self,input,ouput):
        super().__init__()
        self.liner=nn.Liner(input,10)
        self.liner1 = nn.Liner(10,output)
    def forward(self,x):
        x=F.Rule(self.liner(x))
        return self.liner1(x)
    def init_wight(self):
        for param in self.Module():
            if isinstance(param,nn.Linear):
                nn.init.kaiming_normal(param.weight)
```



```python
# 初始化方法2
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
```



```python
# 初始化方法3
torch.save(net.state_dict(),'./model_param.plt')
net.load('./model_param.plt')
```



### 模型微调初始化参数

```python
from torchvision.models import resnet34
from torchvision.models.resnet import BasicBlock
from torch import optim

net = resnet34(pretrained=True)  # 此处也可以通过load存储的模型参数
LR = 0.01
# 初始化指定层
for param in net.modules():
    if isinstance(param, BasicBlock):
        nn.init.kaiming_normal(param.conv1.weight)
        nn.init.kaiming_normal(param.conv1.bias)

# 设置学习率为0或者其他值
fc_params_id = list(map(id, net.fc.parameters()))
other_params = filter(lambda p: id(p) not in fc_params_id, net.parameters())
optimizer = optim.SGD(
    [{"params": other_params, "lr": 0}, {"params": net.fc.parameters(), "lr": LR}],
    momentum=0.09
)

# 冻结学习率
for param in other_params:
    param.requires_grad = False
```


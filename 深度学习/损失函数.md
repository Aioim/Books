在 PyTorch 中,有多种常用的损失函数可供选择,根据不同的任务和需求使用。以下是一些常见的 PyTorch 损失函数及其使用实例:

1. **Mean Squared Error (MSE) Loss**:

   - 适用于回归问题,目标是最小化预测值与真实值之间的平方差。

   - 代码示例:

     ```python
     import torch.nn.functional as F
     
     criterion = F.mse_loss()
     loss = criterion(outputs, targets)
     ```

2. **Cross Entropy Loss**:

   - 适用于分类问题,目标是最小化预测概率分布与真实标签分布之间的交叉熵。

   - 代码示例:

     ```python
     import torch.nn as nn
     
     criterion = nn.CrossEntropyLoss()
     loss = criterion(outputs, targets)
     ```

3. **Negative Log Likelihood (NLL) Loss**:

   - 适用于分类问题,目标是最大化对正确类别的预测概率。

   - 代码示例:

     

     ```python
     import torch.nn.functional as F
     
     criterion = F.nll_loss()
     loss = criterion(log_probs, targets)
     ```

4. **Binary Cross Entropy (BCE) Loss**:

   - 适用于二分类问题,目标是最小化预测概率与真实标签之间的二进制交叉熵。

   - 代码示例:

     ```python
     import torch.nn.functional as F
     
     criterion = F.binary_cross_entropy()
     loss = criterion(outputs, targets)
     ```

5. **Focal Loss**:

   - 适用于类别不平衡的分类问题,通过调整损失函数来提高模型对难分类样本的学习效果。

   - 代码示例:

     ```python
     import torch.nn.functional as F
     
     def focal_loss(outputs, targets, gamma=2, alpha=0.5):
         bce_loss = F.binary_cross_entropy_with_logits(outputs, targets, reduction='none')
         pt = torch.exp(-bce_loss)
         focal_loss = (1 - pt) ** gamma * bce_loss
         return focal_loss.mean()
     
     criterion = focal_loss
     loss = criterion(outputs, targets)
     ```

6. **Smooth L1 Loss (Huber Loss)**:

   - 适用于回归问题,目标是最小化预测值与真实值之间的 Huber 损失。在训练过程中可以提供更稳定的梯度。

   - 代码示例:

     ```python
     import torch.nn as nn
     
     criterion = nn.SmoothL1Loss()
     loss = criterion(outputs, targets)
     ```

这些只是 PyTorch 中常见的几种损失函数,实际使用时可根据具体任务选择合适的损失函数。在训练模型时,通过最小化损失函数,可以不断优化模型的参数,提高模型的性能。